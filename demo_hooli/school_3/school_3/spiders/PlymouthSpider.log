2018-03-02 09:39:11 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: school_3)
2018-03-02 09:39:11 [scrapy.utils.log] INFO: Overridden settings: {'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['school_3.spiders'], 'CONCURRENT_REQUESTS': 32, 'BOT_NAME': 'school_3', 'NEWSPIDER_MODULE': 'school_3.spiders', 'LOG_FILE': 'PlymouthSpider.log'}
2018-03-02 09:39:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats']
2018-03-02 09:39:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-03-02 09:39:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-03-02 09:39:12 [scrapy.middleware] INFO: Enabled item pipelines:
['school_3.pipelines.HooliPipeline']
2018-03-02 09:39:12 [scrapy.core.engine] INFO: Spider opened
2018-03-02 09:39:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-03-02 09:43:01 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: school_3)
2018-03-02 09:43:01 [scrapy.utils.log] INFO: Overridden settings: {'SPIDER_MODULES': ['school_3.spiders'], 'CONCURRENT_REQUESTS': 32, 'BOT_NAME': 'school_3', 'NEWSPIDER_MODULE': 'school_3.spiders', 'LOG_FILE': 'PlymouthSpider.log', 'LOG_LEVEL': 'INFO'}
2018-03-02 09:43:01 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-03-02 09:43:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-03-02 09:43:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-03-02 09:43:01 [scrapy.middleware] INFO: Enabled item pipelines:
['school_3.pipelines.HooliPipeline']
2018-03-02 09:43:01 [scrapy.core.engine] INFO: Spider opened
2018-03-02 09:43:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-03-02 09:43:09 [scrapy.core.engine] INFO: Closing spider (finished)
2018-03-02 09:43:09 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 857,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 14263,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'dupefilter/filtered': 31,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 3, 2, 1, 43, 9, 407710),
 'log_count/INFO': 7,
 'request_depth_max': 2,
 'response_received_count': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2018, 3, 2, 1, 43, 1, 914282)}
2018-03-02 09:43:09 [scrapy.core.engine] INFO: Spider closed (finished)
2018-03-16 16:41:07 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: school_3)
2018-03-16 16:41:07 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'school_3', 'CONCURRENT_REQUESTS': 32, 'SPIDER_MODULES': ['school_3.spiders'], 'LOG_FILE': 'PlymouthSpider.log', 'NEWSPIDER_MODULE': 'school_3.spiders', 'LOG_LEVEL': 'INFO'}
2018-03-16 16:41:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats']
2018-03-16 16:41:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-03-16 16:41:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-03-16 16:41:08 [scrapy.middleware] INFO: Enabled item pipelines:
['school_3.pipelines.HooliPipeline']
2018-03-16 16:41:08 [scrapy.core.engine] INFO: Spider opened
2018-03-16 16:41:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-03-16 16:42:52 [scrapy.extensions.logstats] INFO: Crawled 26 pages (at 26 pages/min), scraped 9 items (at 9 items/min)
2018-03-16 16:44:06 [scrapy.extensions.logstats] INFO: Crawled 26 pages (at 0 pages/min), scraped 15 items (at 6 items/min)
2018-03-16 16:45:12 [scrapy.extensions.logstats] INFO: Crawled 26 pages (at 0 pages/min), scraped 24 items (at 9 items/min)
2018-03-16 16:50:22 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: school_3)
2018-03-16 16:50:22 [scrapy.utils.log] INFO: Overridden settings: {'CONCURRENT_REQUESTS': 32, 'LOG_FILE': 'PlymouthSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'school_3.spiders', 'SPIDER_MODULES': ['school_3.spiders'], 'BOT_NAME': 'school_3'}
2018-03-16 16:50:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-03-16 16:50:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-03-16 16:50:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-03-16 16:50:22 [scrapy.middleware] INFO: Enabled item pipelines:
['school_3.pipelines.HooliPipeline']
2018-03-16 16:50:22 [scrapy.core.engine] INFO: Spider opened
2018-03-16 16:50:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-03-16 16:51:11 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: school_3)
2018-03-16 16:51:11 [scrapy.utils.log] INFO: Overridden settings: {'CONCURRENT_REQUESTS': 32, 'NEWSPIDER_MODULE': 'school_3.spiders', 'LOG_FILE': 'PlymouthSpider.log', 'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['school_3.spiders'], 'BOT_NAME': 'school_3'}
2018-03-16 16:51:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-03-16 16:51:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-03-16 16:51:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-03-16 16:51:12 [scrapy.middleware] INFO: Enabled item pipelines:
['school_3.pipelines.HooliPipeline']
2018-03-16 16:51:12 [scrapy.core.engine] INFO: Spider opened
2018-03-16 16:51:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-03-16 16:53:17 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: school_3)
2018-03-16 16:53:17 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'school_3', 'CONCURRENT_REQUESTS': 32, 'NEWSPIDER_MODULE': 'school_3.spiders', 'SPIDER_MODULES': ['school_3.spiders'], 'LOG_LEVEL': 'INFO', 'LOG_FILE': 'PlymouthSpider.log'}
2018-03-16 16:53:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats']
2018-03-16 16:53:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-03-16 16:53:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-03-16 16:53:18 [scrapy.middleware] INFO: Enabled item pipelines:
['school_3.pipelines.HooliPipeline']
2018-03-16 16:53:18 [scrapy.core.engine] INFO: Spider opened
2018-03-16 16:53:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-03-16 16:54:58 [scrapy.extensions.logstats] INFO: Crawled 19 pages (at 19 pages/min), scraped 9 items (at 9 items/min)
2018-03-16 16:56:21 [scrapy.extensions.logstats] INFO: Crawled 27 pages (at 8 pages/min), scraped 17 items (at 8 items/min)
2018-03-16 16:58:23 [scrapy.extensions.logstats] INFO: Crawled 27 pages (at 0 pages/min), scraped 29 items (at 12 items/min)
2018-03-16 17:00:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.newcastle.edu.au/degrees/bachelor-of-aboriginal-professional-practice> (referer: None)
Traceback (most recent call last):
  File "D:\python03\pachong1030\vir\py3env\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "D:\python03\pachong1030\vir\py3env\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "D:\python03\pachong1030\vir\py3env\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
TimeoutError: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\python03\pachong1030\vir\py3env\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "D:\python03\pachong1030\vir\py3env\lib\site-packages\urllib3\connectionpool.py", line 346, in _make_request
    self._validate_conn(conn)
  File "D:\python03\pachong1030\vir\py3env\lib\site-packages\urllib3\connectionpool.py", line 850, in _validate_conn
    conn.connect()
  File "D:\python03\pachong1030\vir\py3env\lib\site-packages\urllib3\connection.py", line 284, in connect
    conn = self._new_conn()
  File "D:\python03\pachong1030\vir\py3env\lib\site-packages\urllib3\connection.py", line 150, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.VerifiedHTTPSConnection object at 0x0000000006F52198>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\python03\pachong1030\vir\py3env\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "D:\python03\pachong1030\vir\py3env\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "D:\python03\pachong1030\vir\py3env\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.newcastle.edu.au', port=443): Max retries exceeded with url: /degrees/bachelor-of-aboriginal-professional-practice/what-you-will-study/majors/german (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x0000000006F52198>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\python03\pachong1030\vir\py3env\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "D:\python03\pachong1030\vir\py3env\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "D:\python03\pachong1030\vir\py3env\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "D:\python03\pachong1030\vir\py3env\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\python03\pachong1030\vir\py3env\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\demo_liuxue\demo_hooli\school_3\school_3\spiders\Newcastle_ug.py", line 242, in parse
    self.parse_overview(programme_urls,item)
  File "D:\demo_liuxue\demo_hooli\school_3\school_3\spiders\Newcastle_ug.py", line 344, in parse_overview
    data = requests.get(programme_urls)
  File "D:\python03\pachong1030\vir\py3env\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "D:\python03\pachong1030\vir\py3env\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "D:\python03\pachong1030\vir\py3env\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "D:\python03\pachong1030\vir\py3env\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "D:\python03\pachong1030\vir\py3env\lib\site-packages\requests\adapters.py", line 508, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.newcastle.edu.au', port=443): Max retries exceeded with url: /degrees/bachelor-of-aboriginal-professional-practice/what-you-will-study/majors/german (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x0000000006F52198>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。',))
2018-03-16 17:00:22 [scrapy.extensions.logstats] INFO: Crawled 27 pages (at 0 pages/min), scraped 39 items (at 10 items/min)
2018-03-16 17:02:12 [scrapy.extensions.logstats] INFO: Crawled 27 pages (at 0 pages/min), scraped 48 items (at 9 items/min)
2018-03-16 17:02:39 [scrapy.extensions.logstats] INFO: Crawled 27 pages (at 0 pages/min), scraped 52 items (at 4 items/min)
2018-03-16 17:03:31 [scrapy.extensions.logstats] INFO: Crawled 27 pages (at 0 pages/min), scraped 58 items (at 6 items/min)
2018-03-16 17:04:35 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.newcastle.edu.au/degrees/bachelor-of-development-studies-laws-honours>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-03-16 17:04:35 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.newcastle.edu.au/degrees/bachelor-of-development-studies-business>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-03-16 17:04:35 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.newcastle.edu.au/degrees/bachelor-of-development-studies-social-science>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-03-16 17:04:35 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.newcastle.edu.au/degrees/bachelor-of-development-studies-honours>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-03-16 17:04:35 [scrapy.extensions.logstats] INFO: Crawled 27 pages (at 0 pages/min), scraped 64 items (at 6 items/min)
2018-03-16 17:04:50 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.newcastle.edu.au/degrees/bachelor-of-design-architecture>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-03-16 17:04:50 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.newcastle.edu.au/degrees/bachelor-of-education-early-childhood-and-primary>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-03-16 17:05:13 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.newcastle.edu.au/degrees/bachelor-of-development-studies>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-03-16 17:05:13 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.newcastle.edu.au/degrees/bachelor-of-education-early-childhood-and-primary-honours>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-03-16 17:05:27 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.newcastle.edu.au/degrees/bachelor-of-engineering-honours-chemical>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-03-16 17:05:27 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.newcastle.edu.au/degrees/bachelor-of-education-primary-honours>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-03-16 17:05:27 [scrapy.extensions.logstats] INFO: Crawled 27 pages (at 0 pages/min), scraped 70 items (at 6 items/min)
2018-03-16 17:05:40 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.newcastle.edu.au/degrees/bachelor-of-engineering-honours-chemical-business>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-03-16 17:05:40 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.newcastle.edu.au/degrees/bachelor-of-education-primary>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-03-16 17:05:40 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.newcastle.edu.au/degrees/bachelor-of-education-secondary-honours>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-03-16 17:05:40 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.newcastle.edu.au/degrees/bachelor-of-education-secondary>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-03-16 17:06:18 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.newcastle.edu.au/degrees/bachelor-of-engineering-honours-civil>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-03-16 17:06:18 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.newcastle.edu.au/degrees/bachelor-of-engineering-honours-chemical-mathematics>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-03-16 17:06:40 [scrapy.extensions.logstats] INFO: Crawled 27 pages (at 0 pages/min), scraped 78 items (at 8 items/min)
2018-03-16 17:06:40 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.newcastle.edu.au/degrees/bachelor-of-engineering-honours-chemical-science>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-03-16 17:06:40 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.newcastle.edu.au/degrees/bachelor-of-engineering-honours-civil-environmental>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-03-16 17:06:40 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.newcastle.edu.au/degrees/bachelor-of-engineering-honours-civil-business>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-03-16 17:07:24 [scrapy.extensions.logstats] INFO: Crawled 76 pages (at 49 pages/min), scraped 83 items (at 5 items/min)
2018-03-16 17:08:32 [scrapy.extensions.logstats] INFO: Crawled 115 pages (at 39 pages/min), scraped 90 items (at 7 items/min)
2018-03-16 17:09:35 [scrapy.extensions.logstats] INFO: Crawled 131 pages (at 16 pages/min), scraped 96 items (at 6 items/min)
2018-03-16 17:10:34 [scrapy.extensions.logstats] INFO: Crawled 138 pages (at 7 pages/min), scraped 102 items (at 6 items/min)
2018-03-16 17:11:18 [scrapy.extensions.logstats] INFO: Crawled 141 pages (at 3 pages/min), scraped 107 items (at 5 items/min)
2018-03-16 17:11:25 [scrapy.core.engine] INFO: Closing spider (finished)
2018-03-16 17:11:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 71,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 71,
 'downloader/request_bytes': 72668,
 'downloader/request_count': 265,
 'downloader/request_method_count/GET': 265,
 'downloader/response_bytes': 3247108,
 'downloader/response_count': 194,
 'downloader/response_status_count/200': 167,
 'downloader/response_status_count/301': 27,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 3, 16, 9, 11, 25, 842672),
 'item_scraped_count': 107,
 'log_count/ERROR': 20,
 'log_count/INFO': 22,
 'response_received_count': 167,
 'retry/count': 52,
 'retry/max_reached': 19,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 52,
 'scheduler/dequeued': 265,
 'scheduler/dequeued/memory': 265,
 'scheduler/enqueued': 265,
 'scheduler/enqueued/memory': 265,
 'spider_exceptions/ConnectionError': 1,
 'start_time': datetime.datetime(2018, 3, 16, 8, 53, 18, 196462)}
2018-03-16 17:11:25 [scrapy.core.engine] INFO: Spider closed (finished)
